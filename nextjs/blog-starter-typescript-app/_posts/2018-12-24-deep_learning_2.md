---
title: '機械学習の基礎的な知識についてざっくりとしたまとめ'
date: '2018-12-24'
tags: [python, DL, book]
path: blog/deep-learning-2
cover: ./preview.png
excerpt: 書籍 深層学習 を読みながらまとめていく
author:
  name: JJ Kasper
  picture: '/assets/blog/authors/jj.jpeg'
ogImage:
  url: '/assets/blog/dynamic-routing/cover.jpg'
coverImage: '/assets/blog/dynamic-routing/cover.jpg'
---
# 目次


# (学術的な)機械学習の基礎

現在、深層学習について勉強中です。
読んでいる書籍は以下になります。

[深層学習](https://www.amazon.co.jp/dp/4048930621/)

今回、第一部がようやく終わったということで、少し振り返ります。
第一部では下記エントリーの数学の基礎部分と数値計算の基礎、
最後に機械学習の基礎について学びました。

[機械学習に必要な数学についてざっくりとしたまとめ](/blog/deep-learning-1)

機械学習の基礎についてはkaggleに代表されるような技術的なものでも、
企業で使われているような実際的なものでもなく、
あくまで理論的なものになります。
そのため確率やら統計やらの話ばかりで多少イメージしづらかったですが、
論文等を読む際のしっかりとした基本ができたかと思います。
自分のために非常にざっくりと忘備録をつけていきます。


# 学習アルゴリズム

### 学習の定義

>「コンピュータプログラムは、性能指標Pで測定されるタスクTにおける性能が
> 経験Eにより改善される場合、そのタスクTのクラスおよび性能指標Pに関して
> 経験Eから学習すると言われている」
>
> Mitchell(1997)

### タスク _T_

一般的なタスクを列挙する

- 分類
- 欠損値のある入力の分類
- 回帰
- 転写
- 機械翻訳
- 構造出力
- 異常検知
- 合成とサンプリング
- 欠損値補完
- ノイズ除去
- 密度推定

### 性能指標 _p_

機械学習アルゴリズムの能力を評価するためのものが必要である。
例えば、モデルの精度や誤差率などがこれに該当する。
学習に用いた訓練集合とは別のテスト集合を用いて評価する。

### 経験 _E_

機械学習アルゴリズムは大きく *教師あり学習* と *教師なし学習* に
分類されるが、それらの厳格な境界や定義はない。
学習アルゴリズムはデータ集合を経験することで学習する。

### 線形回帰

学習アルゴリズムの中でシンプルで有名な例として線形回帰を取り上げる。

$$
    \hat{y} = \mathbf{w}^{\mathrm{T}} \mathbf{x}
$$

$\mathbf{w}$ は重みと呼ばれるパラメータであり、
$\mathbf{x}$ は特徴量である。
訓練集合において、予測と正解の *平均二乗誤差* を最小化するような重みを求めることで、
データ集合からモデルが経験するということになる。
線形の簡単なモデルを扱っているため、微分方程式が正規方程式として知られる形になり、
一意に解が求まることがわかっている。

$$
    \hat{y} = \mathbf{w}^{\mathrm{T}} \mathbf{x} + b
$$

上記のようにインターセプト項 $b$ を加えたものを線形回帰として扱うこともある。
この場合、 $b$ はバイアスと呼ばれる。統計のバイアスとは異なる。

# 容量、過剰適合、歌唱適合

この章は汎化に対する話となる。
訓練集合から訓練誤差が最小になるように学習を行うのに対し、
汎化性能を測定するためにはテスト集合から汎化誤差(テスト誤差)を計算する。
訓練集合とテスト集合についてはi.i.d.仮定をおいた上で、学習アルゴリズムが目指すことは

1. 訓練誤差を小さくする
2. 訓練誤差とテスト誤差の差を小さくする

この二つの要素は機械学習における二つの中心的な課題に相当し、
それぞれ *過少適合* 、 *過剰適合* と呼ばれる。
これらの度合いは *モデルの容量* を変化させることで制御する。
任意の高い容量という最も極端な状態に到達するには、 *ノンパラメトリックモデル*
の概念を導入する。パラメータが存在しないため、モデルの容量を縛る要素が存在しないモデルである。

### ノーフリーランチ定理

データを生成する分布全てを平均すると、
どのアルゴリズムも過去に観測されていない点を分類する際の誤差率は同じになる
という定理。
全ての分布を平均した場合、他の機械学習アルゴリズムよりも普遍的に良いと言える
機械学習アルゴリズムは存在しないと主張している。

### 正則化

モデルの容量を変更する以外で過少適合、過剰適合を制御する方法として *正則化*
があげられる。
正則化とは他の解に対して優先度を表現する方法の総称である。
一般的には正則化項と呼ばれるペナルティをコスト関数に追加することでモデルを正則化する。

# ハイパーパラメータと検証集合

ほとんどの機械学習アルゴリズムにおいて、挙動を制御するための設定値が存在し、
それらは *ハイパーパラメータ* と呼ばれる。
最適なハイパーパラメータを選ぶために *検証集合* が必要になる。
検証集合は訓練データから構成される。

ほとんどのデータ集合は訓練集合とテスト集合に分けられているが、
その中の訓練集合を訓練集合と検証集合に分割する。
訓練集合でモデルを学習、検証集合でハイパーパラメータの最適化を行い、
テスト集合でモデルの汎化性能を測定する。

# 推定量、バイアス、バリアンス

### 点推定量

関心のある量について「最良の」予測を一つ提示する試みである。

### バイアス

$$
    \mathrm{bias}(\hat{\theta}_m) = \mathbf{E}(\hat{\theta}_m) - \theta
$$

$\mathrm{bias}(\hat{\theta_m}) = 0$ になるものは不偏と呼ばれ、好まれる推定量である。

### 分散と標準誤差

推定量のバリアンスとは単に推定量の分散である。
推定量の分散や標準誤差は、潜在的なデータ生成過程からデータ集合を別々に再サンプリングする際に、
データから計算した推定量がどのように変化するかを示す尺度を提供する。

### 平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ

バイアスは関数やパラメータの真の値からの期待偏差を測定する。
バリアンスはデータのサンプル化の方法に起因すると考えられる期待推定力の偏差を測定する。
平均二乗誤差はこれら二つの誤差を組み込んだものである。

$$
    \mathrm{MSE} = \mathbf{E}[(\hat{\theta}_m - \theta)^2] =
    \mathrm{bias}(\hat{\theta}_m)^2 + \mathrm{Var}(\hat{\theta}_m)
$$

バイアスとバリアンスの関係は、モデルの容量や過少適合、過剰適合という機械学習の概念と
密接に関係している。

# 最尤推定

最尤推定を解釈する方法の一つとして、最尤推定は訓練集合で定義される経験分布とモデル分布の差を
最小化するとみなす方法がある。
最尤法は負の対数尤度(NLL)の最小化、交差エントロピーの最小化、KLダイバージェンスの最大化
として捉えることができる。

# ベイズ統計

点推定に対し、予測を行う際に $\theta$ の取りうる値全てを考慮するものが考えられる。
この手法はベイズ統計の分野となる。
事前確率分布を先験的に仮定し、事例の経験によって、その分布を書き換えていくことで
値の分布を推定する。

### 最大事後確率(MAP)推定

パラメータの完全なベイズ事後分布を用いた推定を行うことが最も理にかなった手法ではあるが、
点推定を行うことが望ましい場合も多い。これを行う方法の一つとしてMAP推定が挙げられる。

# 教師あり学習アルゴリズム

### 確率的教師あり学習

最終的な出力を0~1の値に押し込むことで確率を予測することができる。

### サポートベクトルマシン

線形回帰をベースとして、カーネルトリックと呼ばれる技法を用いることで、
非常に高い性能を発揮した手法。
データ集合が大きい場合、訓練の計算コストが高くなってしまうというデメリットがある。

### その他の教師あり学習アルゴリズム

- k近傍法
- 決定木

# 教師なし学習アルゴリズム

古典的な教師なし学習はデータの、より単純な表現を探す手法ということができる。
より単純なに関する定義として

- より低次元な表現
- 疎な表現
- 独立した表現

の三つが挙げられる。

### 主成分分析(PCA)

元の入力よりも次元が低い表現を学習し、
また成分が互いに線形な相関を持たない表現 (独立ではない)を学習する。
詳細は割愛。

### k平均クラスタリング

k個のクラスタに分割し、疎な表現を実現する。
詳細は割愛。

# 確率的勾配降下法

1回のステップに対し、訓練集合の全てを用いては計算コストが大きくなってしまう。
したがって、アルゴリズムの各ステップにおいて、
訓練集合から一様に抽出されるサンプルのミニバッチをサンプリングし、
それら数百のデータを用いて1回の学習ステップを行うことで計算時間を軽減する。
この手法を確率的勾配降下法と呼び、
更新毎の計算量が訓練集合の大きさに依存しないというメリットを得る。

# 機械学習アルゴリズムの構築

- データ集合の仕様
- コスト関数
- 最適化手順
- モデル

上記4つを組み合わせることで機械学習アルゴリズムは構築される。

# 深層学習の発展を促す課題

### 次元の呪い

次元が大きくなると構成される空間が大きくなりすぎる。

### 局所一様と平滑化

機械学習アルゴリズムにおいて、ある正解データの近傍にあるものは
その正解データと大きく変わらない値に学習されるように、
事前分布が仕込まれていることがほとんである。
その場合、正解データが多く集まっている場所に関しては極めてよく学習できても、
それ以外のデータが集まっていないだけで、同様の分布を持っている場所については
正しく予測できる保証がない。

### 多様体学習

多様体は連結した領域であり、各店の周りの近傍に関連づけられた点の集合である。
ある表現の整理の仕方によって次元が変わるということだと思われる。
