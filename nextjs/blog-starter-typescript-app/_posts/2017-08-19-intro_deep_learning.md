---
title: '機械学習の勉強を始めました'
date: '2017-08-19'
tags: [python, DL, book]
path: blog/intro-deep-learning
cover: ./preview.png
excerpt: ゼロから作る Deep Learning の自分用まとめ
author:
  name: JJ Kasper
  picture: '/assets/blog/authors/jj.jpeg'
ogImage:
  url: '/assets/blog/dynamic-routing/cover.jpg'
coverImage: '/assets/blog/dynamic-routing/cover.jpg'
---
# 目次


# 機械学習の勉強を真面目にはじめました。

今回しっかりと基礎から見直そうと思い、入門書を精読することにしました。
具体的には上記の本を精読し、しっかりとゼロから作っていこうかと。
なんとなく難しかったところとかを適当にまとめたり、シンプルにようやくしたりします。


# ニューラルネットワーク

### 入力ネットワークの構造
入力層、隠れ層、出力層に分けられる。
入力層は例えば、28x28の画像であれば、28x28=784の入力。
隠れ層は何層重ねてもよく、さらにどれだけ縮小しても拡大しても良い。
出力層はクラス分類問題であれば、クラスの数となる。
行列計算と非常に相性が良い構造である。

### 活性化関数

* シグモイド関数
* ステップ関数
* ReLU関数
* ソフトマックス関数
* 恒等関数

これらを回帰問題か分類問題かなどに応じて使い分ける。
主に、重みを付与しながらの層から層への情報伝播の後、
各層で行われる処理はこのような関数による変換。

### バッチ処理
一回に入力する量を増やす処理。
画像であれば、何枚もの画像を別次元に格納する。
この処理により高速化の恩恵が得られる。


# データの学習

### 損失関数
損失関数とは機械学習における目標値である。
損失関数から出力される値を最小化することを目標に機械は学習を行う。
主に用いられる関数として下記のものがあげられる。

* 二乗和誤差
* 交差エントロピー誤差

### 勾配法
極小値にハマり学習の進まないプラトーという状況に陥ることに注意。<br>
毎回の学習の更新量を定めるパラメタを **学習率** と呼ぶ。
学習率は人間があらかじめ設定する必要があるため **ハイパーパラメタ**
と呼ばれる。

### ここまでのまとめ
まとめてみると学習の流れは

1. ミニバッチ
2. 勾配の算出
3. パラメタの更新

これらを繰り返すことになる。この流れを確率的勾配降下法と呼ぶ。
また、ミニパッチ一回分の繰り返しを **1epoch** と呼ぶ

### 過学習
学習に用いたデータセットに特化した学習を行ってしまい汎化能力が失われた状態。
この状態を避けるために学習データとテストデータを分け、
定期的にテストデータの認識精度も測ることで、
両方の認識精度が向上していることを確認する必要がある。

## 誤差逆伝播(backpropagation)
微分値によってパラメタを進める方向、量を決めるが、
微分は一般的に重たい計算のため、高速化するために誤差逆伝播法を用いる。
これは各計算を分解、独立化し、chain ruleによって、
独立した計算それぞれに対する微分を考えていくことで、
行列計算として微分計算を行えると言う利点がある。<br>
隠れ層での行列の形状を変化させる計算をAffineレイヤと呼び、
Affineレイヤを含む、上記の活性化関数それぞれに対し、
独立して計算済みのレイヤを実装していくことで、
部品を組み立てるようにニューラルネットワークを実装することができ、
かつ高速に微分計算を行うことができる。

#### 勾配確認
実装の難しい誤差逆伝播法のバリデーションのため、
実装の簡単な数値微分の結果と比較すること。

## 最適化手法
パラメタの更新のことを最適化と呼ぶ。

* 確率的勾配降下法(SGD)
* Momentum
* AdaGrad
* Adam

上記の4つをこの書籍では取り上げていた。

##### SGD
最もシンプルな手法。
勾配に学習率をかけたものを次のパラメタ更新に用いる。
欠点として、最小値までの傾斜が等方的でない場合、
ジグザグな動きをすることとなり、学習スピードが遅くなる。

##### Momentum
前回の学習スピードを保持することで、
同じ方向に進むときは加速度的に進み、
ジグザグな動きをするときはSGDと比べて減速するようにした。

##### AdaGrad
学習率を定数ではなく変数にする。
過去全ての更新量を記憶し、そのぶん学習率を小さくしていく。
欠点として、学習に長時間かかった場合、学習率が0に近づき、
学習が進まなくなることが挙げられるが、それを改善したRMSProp
と言う手法も存在する。

##### Adam
直感的にはMomentumとAdaGradを組み合わせたようなものらしい

### 重みの初期値

##### 1を標準偏差としたガウス分布
0と1にアクティベーション(各レイヤの出力)が固まることから
勾配消失が起きているとわかる。

##### 0.001を標準偏差としたガウス分布
0.5付近に固まるため、勾配消失は怒っていないが、
その代わり表現力が小さくなっている。
ある値にアクティベーションが集まると、
単一のレイヤで全てのレイヤを表現できてしまうと言う問題や、
学習が効率よくいかない問題が挙げられる。

##### Xavierの初期値
前層のノードの数に準じて初期値を小さくする手法。
活性化関数が線形関数(線形関数に近似される関数)の際に用いられる。

##### Heの初期値
Xavierに2倍の広がりをもたせたもの。
非線形関数に用いられる。

### Batch Normalization
データを綺麗な分布にならす手法。
Affineレイヤと活性化関数レイヤの間に
Batch Normalizationレイヤを入れて使用する。

* 学習スピードの増加
* 初期値に対する依存度の減少
* 過学習の抑制

などの効果がある。

### 正則化
過学習を抑制するための手法

##### Weight decay(荷重減衰)
損失関数に重みに準じた量を加算することで、
大きな重みを持つことにペナルティを課す。

##### Dropout
確率で、ノードを消去することで何かうまいこと過学習が抑制されるらしい。


# 畳み込みニューラルネットワーク(CNN)
主に画像に用いられるニューラルネットワーク。
昔のモデル(ここまで考えてきたモデル)では、
Affineレイヤで前層の出力を全結合していたが、
その部分をConvolutionレイヤで置き換え、
必要に応じて、活性化関数適用後の出力にPoolingレイヤを挟むことで
Affineレイヤの形状無視と言う欠点を補う。

### Convolutionレイヤ
形状を考えるために、入力データに対して、
**フィルター(カーネル)** を用いた結合を行う。
フィルターは入力データと同じチャンネル数で、任意のサイズを持つ。
フィルターのサイズと、
入力データに対する **パティング** 、フィルターの **ストライド** によって
出力データのサイズが決定する。
一つのフィルターに対して、出力データのチャンネル数は常に1つのため、
複数のフィルターを用意して、出力データのチャンネル数を調整する。

### Poolingレイヤ
サイズを小さくするための演算。
ある範囲の最も大きな値のみ抽出していくMaxPoolingが画像処理の分野では主流である。
Poolingレイヤの特徴は以下の3つに代表される。

* 学習するパラメータがない
* チャンネル数は変化しない
* 微小な位置変化に対してロバスト

### CNNの特徴？
フィルターが前半の層では低次元のエッジなどを抽出していくのに対し、
後半のフィルターは高次元の犬の顔などを抽出し始めるらしい。
これは層を多くすることによって、出力までの下準備をそれまでのレイヤで
丹念にできることに起因するとかしないとか。

### 代表的なCNN

##### LeNet
1998年初めてのCNN？基本的な構造は今使われているものと同じだが、
PoolingレイヤでMaxPoolingを行なっているわけではないらしいのと、
活性化関数がSigmoid関数らしい。

##### AlexNet
2012年に彗星のごとくコンペティションに現れ、
圧倒的な成績でトロフィーと話題をさらっていったAlexNetさんです。
機械学習ブームの火付け役をしてくれたらしいですね。LeNetと比べると

* 活性化関数にReLU関数を用いる
* LRNと言う局所的正規化を行う関数を挟む
* Dropoutを使用する

松尾教授の書籍を読んだ時、うろ覚えですが、確かこのDropoutが
革新的だったといっていたような気がします。うろ覚えなので突っ込まないでください。


# ディープラーニング

### 層を厚くすると言うこと

* 表現力の増加
* 表現するために必要な学習パラメタの減少
* 学習時間は増加

こんな感じのメリットデメリットがあるらしい。
表現力の増加とあるが、実際、MNISTの認識では2層くらいのモデルが
最も高精度らしく、表現力がどの程度必要なのかも考えなければならない。
ちなみに2015年のクラス分類コンペティションの優勝モデルは
150層とか言う馬鹿げたレベルのディープさだったそうな。

### 有名なネットワーク

##### VGG
CNNの基本型らしい。3x3の小さなフィルターでなんども畳み込むそうな。

##### GoogleLeNet
基本的に層をディープにするといったら、伝播方向っぽいんだけど、
Googleさんは横方向にも伸ばしてしまったそうな。
**インセプション構造** と呼ぶらしく、複数のサイズのフィルターで
たたみ込んでその結果を結合するそうな。

##### ResNet
GoogleがきてMSが来ないわけがない。
これがさっき言ったアホみたいに層を深くした150層ネットワーク。
レイヤを通した出力とレイヤを通す前の入力の合計をその層の出力とする
スキップ構造と呼ばれるものを用いることで、
勾配消失問題を克服して、層をものごっつ厚くできたそうな。

### 転移学習
すでに学習済みのパラメタをそのまま初期値として用いて、
次の学習を行う手法のこと。少ないデータセットしか手元にない時とかいいらしい。

### 最近ディープラーニングで行われていること

* 物体検出(物体認識の適用範囲検索と物体認識の結合)
* セグメンテーション(ピクセルレベルでのクラス分類)
* 画像キャプション生成(CNNとRNNの結合)
* 画像スタイル変換(中間出力とのloss)
* 画像生成(DCGAN)

### 強化学習
エージェント(computer)が環境から得られる報酬を
最大化するように動いていくのかな？
ゲームとかでやってるらしい。
パックマンをコンピューターにやらせたらもう人は勝てないらしい。
とりあえず強化学習にはまた独自のアルゴリズムがあるっぽくて(Q学習？)
それとCNNを融合させることでDeep Q-Networkとか言うすごい奴が生まれたらしい。

# まとめ
特殊な用語がたくさん出てきたので、
とりあえずこの分野に入るときには一回真面目に入門書を読むべきと感じました。
