<!DOCTYPE html><html lang="ja"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#fdf6e3"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#fdf6e3"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta name="description"/><meta property="og:title" content="機械学習の正則化についてまとめ"/><meta property="og:type" content="website"/><meta property="og:url" content="{DOMAIN}/posts/2019-06-04-deep_learning_5"/><meta property="og:description"/><meta property="og:image" content="https://og-image.vercel.app/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta property="og:image:alt"/><meta property="og:site_name" content="dondakeshimoの丸太"/><meta property="og:locale" content="ja_JP"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="機械学習の正則化についてまとめ"/><meta property="twitter:description"/><meta property="twitter:image" content="https://og-image.vercel.app/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta property="twitter:image:alt"/><meta property="twitter:site" content="dondakeshimo"/><meta property="twitter:creator" content="dondakeshimo"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"/><title>機械学習の正則化についてまとめ | dondakeshimoの丸太</title><meta property="og:image" content="/assets/blog/dynamic-routing/cover.jpg"/><meta name="next-head-count" content="31"/><link rel="preload" href="/_next/static/css/0f9d9436e0b0760d86ae.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0f9d9436e0b0760d86ae.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a54b4f32bdc1ef890ddd.js"></script><script src="/_next/static/chunks/webpack-61095c13c5984b221292.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-c034215587cd157b2989.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d2622b3552023a29a89a.js" defer=""></script><script src="/_next/static/chunks/cb1608f2-854db26f8877d6c8528f.js" defer=""></script><script src="/_next/static/chunks/a9a7754c-ecd5c258dd80f13aa656.js" defer=""></script><script src="/_next/static/chunks/349-e0275456508bca40c658.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-c414199f8f835e5668ae.js" defer=""></script><script src="/_next/static/yNrEyh5A0Oyk3eJQw5kY9/_buildManifest.js" defer=""></script><script src="/_next/static/yNrEyh5A0Oyk3eJQw5kY9/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-vfull"><main><div class="full-container mx-auto px-5"><a class="text-l font-bold text-tight mb-xl mt-l"><a class="logo" href="/"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="hand-point-up" class="svg-inline--fa fa-hand-point-up fa-w-12 svg-l dondake-icon" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M135.652 0c23.625 0 43.826 20.65 43.826 44.8v99.851c17.048-16.34 49.766-18.346 70.944 6.299 22.829-14.288 53.017-2.147 62.315 16.45C361.878 158.426 384 189.346 384 240c0 2.746-.203 13.276-.195 16 .168 61.971-31.065 76.894-38.315 123.731C343.683 391.404 333.599 400 321.786 400H150.261l-.001-.002c-18.366-.011-35.889-10.607-43.845-28.464C93.421 342.648 57.377 276.122 29.092 264 10.897 256.203.008 242.616 0 224c-.014-34.222 35.098-57.752 66.908-44.119 8.359 3.583 16.67 8.312 24.918 14.153V44.8c0-23.45 20.543-44.8 43.826-44.8zM136 416h192c13.255 0 24 10.745 24 24v48c0 13.255-10.745 24-24 24H136c-13.255 0-24-10.745-24-24v-48c0-13.255 10.745-24 24-24zm168 28c-11.046 0-20 8.954-20 20s8.954 20 20 20 20-8.954 20-20-8.954-20-20-20z"></path></svg>dondakeshimoの丸太</a></a><article class="mb-2xl"><h1 class="title text-xl font-bold mb-l text-center">機械学習の正則化についてまとめ</h1><div class="container-80 mx-auto"><div class="mb-l text-l"><time dateTime="2019-06-04">June	4, 2019</time></div></div><div class="container-80 mx-auto"><div><p>かなり回を重ねてきたけど，完全に自分のことしか見ていない.
絶対他の人の役には立っていないとわかる．
そんなブログで突き進む．
前回同様，深層学習について学んだことをまとめる．
今回のテーマは深層学習における正則化である．</p>
<h1 id="目次">目次</h1>
<ul>
<li><a href="#%E6%AD%A3%E5%89%87%E5%8C%96">正則化</a></li>
<li><a href="#%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%8E%E3%83%AB%E3%83%A0%E3%83%9A%E3%83%8A%E3%83%AB%E3%83%86%E3%82%A3">パラメータノルムペナルティ</a>
<ul>
<li>
<ul>
<li><a href="#l2%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E6%AD%A3%E5%89%87%E5%8C%96"><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>パラメータ正則化</a></li>
<li><a href="#l1%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E6%AD%A3%E5%89%87%E5%8C%96"><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">L^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>パラメータ正則化</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E6%9C%80%E9%81%A9%E5%8C%96%E3%81%A8%E3%81%97%E3%81%A6%E3%81%AE%E3%83%8E%E3%83%AB%E3%83%A0%E3%83%9A%E3%83%8A%E3%83%AB%E3%83%86%E3%82%A3">条件付き最適化としてのノルムペナルティ</a></li>
<li><a href="#%E6%AD%A3%E5%89%87%E5%8C%96%E3%81%A8%E5%88%B6%E7%B4%84%E4%B8%8D%E8%B6%B3%E5%95%8F%E9%A1%8C">正則化と制約不足問題</a></li>
<li><a href="#%E3%83%87%E3%83%BC%E3%82%BF%E9%9B%86%E5%90%88%E3%81%AE%E6%8B%A1%E5%BC%B5">データ集合の拡張</a></li>
<li><a href="#%E3%83%8E%E3%82%A4%E3%82%BA%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E9%A0%91%E5%81%A5%E6%80%A7">ノイズに対する頑健性</a>
<ul>
<li>
<ul>
<li><a href="#%E5%87%BA%E5%8A%9B%E7%9B%AE%E6%A8%99%E3%81%B8%E3%81%AE%E3%83%8E%E3%82%A4%E3%82%BA%E3%81%AE%E6%B3%A8%E5%85%A5">出力目標へのノイズの注入</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%8D%8A%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92">半教師あり学習</a></li>
<li><a href="#%E3%83%9E%E3%83%AB%E3%83%81%E3%82%BF%E3%82%B9%E3%82%AF%E5%AD%A6%E7%BF%92">マルチタスク学習</a></li>
<li><a href="#%E6%97%A9%E6%9C%9F%E7%B5%82%E4%BA%86">早期終了</a></li>
<li><a href="#%E3%83%91%E3%83%A9%E3%83%A1%E3%82%BF%E6%8B%98%E6%9D%9F%E3%81%A8%E3%83%91%E3%83%A9%E3%83%A1%E3%82%BF%E5%85%B1%E6%9C%89">パラメタ拘束とパラメタ共有</a>
<ul>
<li>
<ul>
<li><a href="#%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF">畳み込みニューラルネットワーク</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E3%82%B9%E3%83%91%E3%83%BC%E3%82%B9%E8%A1%A8%E7%8F%BE">スパース表現</a></li>
<li><a href="#%E3%83%90%E3%82%AE%E3%83%B3%E3%82%B0%E3%82%84%E3%81%9D%E3%81%AE%E4%BB%96%E3%81%AE%E3%82%A2%E3%83%B3%E3%82%B5%E3%83%B3%E3%83%96%E3%83%AB%E6%89%8B%E6%B3%95">バギングやその他のアンサンブル手法</a></li>
<li><a href="#%E3%83%89%E3%83%AD%E3%83%83%E3%83%97%E3%82%A2%E3%82%A6%E3%83%88">ドロップアウト</a></li>
<li><a href="#%E6%95%B5%E5%AF%BE%E7%9A%84%E5%AD%A6%E7%BF%92">敵対的学習</a></li>
<li><a href="#%E6%8E%A5%E8%B7%9D%E9%9B%A2%EF%BC%8C%E6%8E%A5%E7%B7%9A%E4%BC%9D%E6%92%AD%E6%B3%95%EF%BC%8C%E5%A4%9A%E6%A7%98%E4%BD%93%E6%8E%A5%E5%88%86%E9%A1%9E%E5%99%A8">接距離，接線伝播法，多様体接分類器</a></li>
<li><a href="#%E3%81%BE%E3%81%A8%E3%82%81">まとめ</a></li>
</ul>
<h1 id="正則化">正則化</h1>
<p>深層学習の観点では，正則化手法のほとんどは推定量の正則化に基づいている．
推定量の正則化は，バイアスの増加とバリアンスの減少を引き換えることで機能する．
過度にバイアスを増加させずにバリアンスを大きく減少させる正則化項が望まれる．</p>
<h1 id="パラメータノルムペナルティ">パラメータノルムペナルティ</h1>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>J</mi><mo>~</mo></mover><mo stretchy="false">(</mo><mi mathvariant="bold">θ</mi><mo separator="true">;</mo><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>J</mi><mo stretchy="false">(</mo><mi mathvariant="bold">θ</mi><mo separator="true">;</mo><mi mathvariant="bold">X</mi><mo separator="true">,</mo><mi mathvariant="bold">y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi mathvariant="bold">θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\tilde{J}(\mathbf{\theta}; \mathbf{X}, \mathbf{y}) = J(\mathbf{\theta}; \mathbf{X}, \mathbf{y}) + \alpha \Omega(\mathbf{\theta})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1701899999999998em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9201899999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span><span style="top:-3.6023300000000003em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.08332999999999999em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathbf">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathbf">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span></div>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>α</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\alpha \in [0, \infty]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∞</span><span class="mclose">]</span></span></span></span></span></div>
<p>バイアスパラメータには関与せず重みのみにペナルティを課す．</p>
<h3 id="l2パラメータ正則化"><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>パラメータ正則化</h3>
<p>重み減衰，リッジ回帰，ティホノフ正則化として知られる．
ステップごとの観点で言うと，重みの更新に際して一定の割合
(<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi><mi>α</mi><mo separator="true">,</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">\varepsilon\alpha, \varepsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">ε</span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">ε</span></span></span></span></span>: learning rate)
が間引かれながら学習を進めていく頃になる．
学習全体の観点で言うと，パラメータが目的関数を減少させることに大きく寄与する方向には，
相対的に重み減衰の影響が少ない．
逆に学習において重要でない方向に対応する重みベクトルの要素は
訓練全体で正則化を使うことで減衰する．</p>
<h3 id="l1パラメータ正則化"><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">L^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>パラメータ正則化</h3>
<p>個々のパラメータの絶対値の総和を減衰項とする．
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>正則化と比して<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">L^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>正則化ではいくつかのパラメータの最適値が0になる
スパースな解が得られる．
このスパース性を利用して特徴量選択を行うことができる．</p>
<h1 id="条件付き最適化としてのノルムペナルティ">条件付き最適化としてのノルムペナルティ</h1>
<p>一般化ラグランジュ関数を構築することで制約問題にパラメータペナルティの問題を帰着させると，
パラメータ正則化は重みの解の範囲を限定する作用があることがわかる．
減衰項の係数である <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span> を増減させることで，解の範囲を大まかに制御することができる．
適切な範囲の係数である<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span>がわかっている場合， <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">\alpha = k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span> として
ペナルティを課すよりも，解が範囲を超えたタイミングで <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi mathvariant="bold">θ</mi><mo stretchy="false">)</mo><mo>&#x3C;</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">\Omega(\mathbf{\theta}) \lt k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ω</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&#x3C;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span>
の範囲に再射影する方が最適解ではない極小値に <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span> が陥らず効率的に値を探索できる．</p>
<h1 id="正則化と制約不足問題">正則化と制約不足問題</h1>
<p>正則化を行うことで，サンプルに分散が観察されない時や劣決定系問題を含む場合に
解を定めることができる．</p>
<h1 id="データ集合の拡張">データ集合の拡張</h1>
<p>機械学習モデルの汎化性能を高める最善の方法はより多くのデータで訓練することであり，
そのために偽データを訓練データの一部として用いることで，訓練データを水増しするアプローチがある．
このアプローチは分類において最も簡単な方法となる．
ただし，密度推定タスクなど，他の多くのタスクについても簡単に適用できるものではないことに
留意されたい．</p>
<p>特にデータ集合の拡張が効果を発揮している分野として物体認識がある．
この分野では画像の回転やスケーリングを行うことでデータの拡張を行う．</p>
<p>ノイズを元データに加えると言う手法もデータ拡張の一つとみなせる．
ニューラルネットワークはノイズに対してあまり頑健でないことが証明されている．
ノイズへの頑健性を高めるためには単純にノイズを加えたデータで学習をする手法がある．
入力へのノイズの注入は雑音除去自己符号器のような教師なし学習アルゴリズムの一部である．
2014年にPoole et al.によりノイズの大きさを非常に注意深く調整すれば
この手法が極めて有効であることが証明された．
ドロップアウトもノイズの乗算を用いて新しい入力を構成する皇帝とみなせる．</p>
<p>機械学習アルゴリズムのベンチマークを比較する際は，
スコアを算出した際にデータ拡張が行われていたか確かめることが重要である．
データ集合の拡張の有無によって差が出ている場合，
単純にモデルやアルゴリズムの比較ができないためである．</p>
<h1 id="ノイズに対する頑健性">ノイズに対する頑健性</h1>
<p>一般的に，ノイズの追加は，特にノイズが隠れユニットに加えられた場合に，
単純にパラメータを縮小するよりずっと強力になりうる．</p>
<h3 id="出力目標へのノイズの注入">出力目標へのノイズの注入</h3>
<p>データ集合の拡張で記述したノイズ手法以外に，出力ラベルに対してノイズを含ませる手法も存在する．
このような手法の代表的なものがラベル平滑化と呼ばれる手法である．</p>
<h1 id="半教師あり学習">半教師あり学習</h1>
<p>表現を学習する手法であるらしいがあまり理解できなかった．
Chapelle et al, (2006)に詳細が書いてあるらしい．</p>
<h1 id="マルチタスク学習">マルチタスク学習</h1>
<p><em>異なるタスクに関連づけられているデータで観測される変動を説明する因子の中には，
二つ以上のタスクの間で共有されるものがいくつか存在する</em>
と言う事前信念のもとに適用される．
モデルの一部を異なるタスク間で共有し，重みも共有することで汎化性能を改善する．
このようなモデルのパラメータは</p>
<ol>
<li>タスク固有のパラメータ</li>
<li>異なるタスク間で共有されるパラメータ</li>
</ol>
<p>汎化と汎化誤差の浄化をこれらのモデルによって改善することが可能である．</p>
<h1 id="早期終了">早期終了</h1>
<p>エポック数を回し切った最後のパラメタではなく，
検証誤差が最少となったエポックでのパラメタを返す手法を早期終了という．
早期終了はその単純さと有効性から，深層学習において一般的に最も使われている正則化である．</p>
<p>最大のメリットとして，ハイパーパラメタの一つであるエポック数を排除できることが挙げられる．
逆に，デメリットは訓練中に定期的に検証誤差を測定する必要があるということと，
最良のパラメタのコピーを保持する必要があるという点である．
しかしパラメタの保持についてはメモリに格納する必要はないため，通常無視することができる．</p>
<p>早期終了では検証集合が必要となるため，一部のデータが訓練で使えないこととなる．
これらのデータを使いたい場合，早期終了を行い，最適なエポック数やモデル訓練回数を調べた後に，
同様の回数でもう一度始めから訓練するという手法がある．
この場合，パラメタの更新回数を等しくするのかエポック数を等しくするかの2通りの手法がある．
また，早期終了で得たパラメタを保持したまま追加分のデータを含めたデータ集合で訓練するという
手法もある．
しかし，この手法は一般的に良好な結果を示さない．</p>
<h1 id="パラメタ拘束とパラメタ共有">パラメタ拘束とパラメタ共有</h1>
<p>類似したタスクを，類似した入出力をもつ二つのモデルの重み <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>w</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">w_i^{(A)}, w_i^{(B)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.321664em;vertical-align:-0.27686399999999994em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">A</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231360000000004em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.27686399999999994em;"><span></span></span></span></span></span></span></span></span></span></span>
があるとき，片方の学習時にもう片方の学習済みパラメタに近づけるように
ノルムペナルティを課すことをパラメタ拘束といい，
完全に学習済みのパラメタと同じになるように学習することをパラメタ共有という．</p>
<h3 id="畳み込みニューラルネットワーク">畳み込みニューラルネットワーク</h3>
<p>パラメタ共有最大の活躍の場は間違いなく畳み込みニューラルネットワークである．
特に画像認識において1ピクセルのずれても猫は猫である．
そのため，入力全体に対して同様の重みを持つ隠れユニットを用いて計算することが有効となる．</p>
<h1 id="スパース表現">スパース表現</h1>
<p>下記のような正則化を用いることでパラメタのスパース表現が可能になる．</p>
<ul>
<li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">L^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> ノルムペナルティ</li>
<li>スチューデントのt事前分布から導かれたペナルティ</li>
<li>KLダイバージェンスペナルティ</li>
<li>直行マッチング追跡</li>
</ul>
<h1 id="バギングやその他のアンサンブル手法">バギングやその他のアンサンブル手法</h1>
<p>バギング(Bootstrap AGGregatING)はいくつかのモデルを組み合わせることで汎化誤差を
減少させる手法である．
モデル平均化と呼ばれる機械学習の一般的な手法や．
アンサンブル手法などが含まれる？</p>
<p>(この辺りの関係はイマイチわかっていないので後で調べるかもしれない)</p>
<p>モデル平均化は汎化誤差を削減する手法としては極めて強力で信頼できるものである．</p>
<h1 id="ドロップアウト">ドロップアウト</h1>
<p><strong>ドロップアウトは最強である</strong></p>
<p>ドロップアウトは大規模なニューラルネットワークに対して
バギングを実用的に行う手法であると考えられる．
具体的な手法はランダムに隠れユニットからの出力が0になる場所を作成する．
そして，出力のないユニットが覗かれたランダムなモデルを含んだアンサンブル学習を
近似的に行っていると見做すことができる．</p>
<p>ドロップアウトは重み減衰やスパース活動正則化といった標準的で
計算コストの低い他の正則化手法より効果的であることが示されている．
ただし，訓練事例がニューラルネットワークに用いる標準的な量と比して，
極端に小さい場合はその限りではないという結果も得られている．</p>
<p>また，ドロップアウトを用いることで各隠れユニットは他の隠れユニットが
どのようなものであろうとも性能を発揮できるように学習されるという利点もある．</p>
<h1 id="敵対的学習">敵対的学習</h1>
<p>ニューラルネットワークは高い線型性を持っているため，入力値の小さな変化に対して
大きく反応してしまうことがある．
例えばパンダの画像にノイズを混ぜると，
人間はその画像をただのパンダの画像としか認知できないが，
元の画像ではパンダと認識できていた深層学習モデルはそれをテナガザルと推論することもある．
この事例を逆手に取り，例えばノイズを加えた画像を訓練事例に含めていくことで
誤り率を減少させることが可能である．</p>
<h1 id="接距離，接線伝播法，多様体接分類器">接距離，接線伝播法，多様体接分類器</h1>
<p>次元の呪いを克服するため，機械学習の分野では多様体仮説を元に学習を行うことが多い．
接距離アルゴリズムと接線伝播アルゴリズムはその仮説をそのままアルゴリズムに持ち込む．
特に接線伝播法はデータ集合拡張や二重逆伝播法，敵対的学習などと関係する．</p>
<h1 id="まとめ">まとめ</h1>
<p><em>ドロップアウトと早期学習を組み合わせておけばなんかうまい感じによしなになりそうな説！</em></p></div></div></article></div></main></div><footer><div class="full-container mx-auto px-5"><div class="full-container mx-auto px-5"><div class="py-m sns-container my-l"><div class="mx-s github-icon"><a href="https://github.com/dondakeshimo"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="mx-s twitter-icon"><a href="https://twitter.com/dondakeshimo"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="twitter" class="svg-inline--fa fa-twitter fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"機械学習の正則化についてまとめ","date":"2019-06-04","slug":"2019-06-04-deep_learning_5","author":{"name":"JJ Kasper","picture":"/assets/blog/authors/jj.jpeg"},"content":"\u003cp\u003eかなり回を重ねてきたけど，完全に自分のことしか見ていない.\n絶対他の人の役には立っていないとわかる．\nそんなブログで突き進む．\n前回同様，深層学習について学んだことをまとめる．\n今回のテーマは深層学習における正則化である．\u003c/p\u003e\n\u003ch1 id=\"目次\"\u003e目次\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E6%AD%A3%E5%89%87%E5%8C%96\"\u003e正則化\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%8E%E3%83%AB%E3%83%A0%E3%83%9A%E3%83%8A%E3%83%AB%E3%83%86%E3%82%A3\"\u003eパラメータノルムペナルティ\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#l2%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E6%AD%A3%E5%89%87%E5%8C%96\"\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003eパラメータ正則化\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#l1%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E6%AD%A3%E5%89%87%E5%8C%96\"\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003eパラメータ正則化\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E6%9C%80%E9%81%A9%E5%8C%96%E3%81%A8%E3%81%97%E3%81%A6%E3%81%AE%E3%83%8E%E3%83%AB%E3%83%A0%E3%83%9A%E3%83%8A%E3%83%AB%E3%83%86%E3%82%A3\"\u003e条件付き最適化としてのノルムペナルティ\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%AD%A3%E5%89%87%E5%8C%96%E3%81%A8%E5%88%B6%E7%B4%84%E4%B8%8D%E8%B6%B3%E5%95%8F%E9%A1%8C\"\u003e正則化と制約不足問題\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%87%E3%83%BC%E3%82%BF%E9%9B%86%E5%90%88%E3%81%AE%E6%8B%A1%E5%BC%B5\"\u003eデータ集合の拡張\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%8E%E3%82%A4%E3%82%BA%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E9%A0%91%E5%81%A5%E6%80%A7\"\u003eノイズに対する頑健性\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E5%87%BA%E5%8A%9B%E7%9B%AE%E6%A8%99%E3%81%B8%E3%81%AE%E3%83%8E%E3%82%A4%E3%82%BA%E3%81%AE%E6%B3%A8%E5%85%A5\"\u003e出力目標へのノイズの注入\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%8D%8A%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92\"\u003e半教師あり学習\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%9E%E3%83%AB%E3%83%81%E3%82%BF%E3%82%B9%E3%82%AF%E5%AD%A6%E7%BF%92\"\u003eマルチタスク学習\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%97%A9%E6%9C%9F%E7%B5%82%E4%BA%86\"\u003e早期終了\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%91%E3%83%A9%E3%83%A1%E3%82%BF%E6%8B%98%E6%9D%9F%E3%81%A8%E3%83%91%E3%83%A9%E3%83%A1%E3%82%BF%E5%85%B1%E6%9C%89\"\u003eパラメタ拘束とパラメタ共有\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF\"\u003e畳み込みニューラルネットワーク\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%82%B9%E3%83%91%E3%83%BC%E3%82%B9%E8%A1%A8%E7%8F%BE\"\u003eスパース表現\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%90%E3%82%AE%E3%83%B3%E3%82%B0%E3%82%84%E3%81%9D%E3%81%AE%E4%BB%96%E3%81%AE%E3%82%A2%E3%83%B3%E3%82%B5%E3%83%B3%E3%83%96%E3%83%AB%E6%89%8B%E6%B3%95\"\u003eバギングやその他のアンサンブル手法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%89%E3%83%AD%E3%83%83%E3%83%97%E3%82%A2%E3%82%A6%E3%83%88\"\u003eドロップアウト\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%95%B5%E5%AF%BE%E7%9A%84%E5%AD%A6%E7%BF%92\"\u003e敵対的学習\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%8E%A5%E8%B7%9D%E9%9B%A2%EF%BC%8C%E6%8E%A5%E7%B7%9A%E4%BC%9D%E6%92%AD%E6%B3%95%EF%BC%8C%E5%A4%9A%E6%A7%98%E4%BD%93%E6%8E%A5%E5%88%86%E9%A1%9E%E5%99%A8\"\u003e接距離，接線伝播法，多様体接分類器\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003eまとめ\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"正則化\"\u003e正則化\u003c/h1\u003e\n\u003cp\u003e深層学習の観点では，正則化手法のほとんどは推定量の正則化に基づいている．\n推定量の正則化は，バイアスの増加とバリアンスの減少を引き換えることで機能する．\n過度にバイアスを増加させずにバリアンスを大きく減少させる正則化項が望まれる．\u003c/p\u003e\n\u003ch1 id=\"パラメータノルムペナルティ\"\u003eパラメータノルムペナルティ\u003c/h1\u003e\n\u003cdiv class=\"math math-display\"\u003e\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003cmo\u003e~\u003c/mo\u003e\u003c/mover\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eθ\u003c/mi\u003e\u003cmo separator=\"true\"\u003e;\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eX\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003ey\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eJ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eθ\u003c/mi\u003e\u003cmo separator=\"true\"\u003e;\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eX\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003ey\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003eΩ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eθ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\tilde{J}(\\mathbf{\\theta}; \\mathbf{X}, \\mathbf{y}) = J(\\mathbf{\\theta}; \\mathbf{X}, \\mathbf{y}) + \\alpha \\Omega(\\mathbf{\\theta})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.1701899999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9201899999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.09618em;\"\u003eJ\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.6023300000000003em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.08332999999999999em;\"\u003e\u003cspan class=\"mord\"\u003e~\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e;\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003eX\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\" style=\"margin-right:0.01597em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.09618em;\"\u003eJ\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e;\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003eX\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\" style=\"margin-right:0.01597em;\"\u003ey\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003cspan class=\"mord\"\u003eΩ\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\"math math-display\"\u003e\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmo\u003e∈\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi mathvariant=\"normal\"\u003e∞\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha \\in [0, \\infty]\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e∈\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e∞\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/div\u003e\n\u003cp\u003eバイアスパラメータには関与せず重みのみにペナルティを課す．\u003c/p\u003e\n\u003ch3 id=\"l2パラメータ正則化\"\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003eパラメータ正則化\u003c/h3\u003e\n\u003cp\u003e重み減衰，リッジ回帰，ティホノフ正則化として知られる．\nステップごとの観点で言うと，重みの更新に際して一定の割合\n(\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eε\u003c/mi\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003eε\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\varepsilon\\alpha, \\varepsilon\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eε\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eε\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e: learning rate)\nが間引かれながら学習を進めていく頃になる．\n学習全体の観点で言うと，パラメータが目的関数を減少させることに大きく寄与する方向には，\n相対的に重み減衰の影響が少ない．\n逆に学習において重要でない方向に対応する重みベクトルの要素は\n訓練全体で正則化を使うことで減衰する．\u003c/p\u003e\n\u003ch3 id=\"l1パラメータ正則化\"\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003eパラメータ正則化\u003c/h3\u003e\n\u003cp\u003e個々のパラメータの絶対値の総和を減衰項とする．\n\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e正則化と比して\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e正則化ではいくつかのパラメータの最適値が0になる\nスパースな解が得られる．\nこのスパース性を利用して特徴量選択を行うことができる．\u003c/p\u003e\n\u003ch1 id=\"条件付き最適化としてのノルムペナルティ\"\u003e条件付き最適化としてのノルムペナルティ\u003c/h1\u003e\n\u003cp\u003e一般化ラグランジュ関数を構築することで制約問題にパラメータペナルティの問題を帰着させると，\nパラメータ正則化は重みの解の範囲を限定する作用があることがわかる．\n減衰項の係数である \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eα\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e を増減させることで，解の範囲を大まかに制御することができる．\n適切な範囲の係数である\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003eがわかっている場合， \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha = k\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.0037em;\"\u003eα\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e として\nペナルティを課すよりも，解が範囲を超えたタイミングで \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eΩ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eθ\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e\u0026#x3C;\u003c/mo\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\Omega(\\mathbf{\\theta}) \\lt k\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003eΩ\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e\u0026#x3C;\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03148em;\"\u003ek\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nの範囲に再射影する方が最適解ではない極小値に \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\theta\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e が陥らず効率的に値を探索できる．\u003c/p\u003e\n\u003ch1 id=\"正則化と制約不足問題\"\u003e正則化と制約不足問題\u003c/h1\u003e\n\u003cp\u003e正則化を行うことで，サンプルに分散が観察されない時や劣決定系問題を含む場合に\n解を定めることができる．\u003c/p\u003e\n\u003ch1 id=\"データ集合の拡張\"\u003eデータ集合の拡張\u003c/h1\u003e\n\u003cp\u003e機械学習モデルの汎化性能を高める最善の方法はより多くのデータで訓練することであり，\nそのために偽データを訓練データの一部として用いることで，訓練データを水増しするアプローチがある．\nこのアプローチは分類において最も簡単な方法となる．\nただし，密度推定タスクなど，他の多くのタスクについても簡単に適用できるものではないことに\n留意されたい．\u003c/p\u003e\n\u003cp\u003e特にデータ集合の拡張が効果を発揮している分野として物体認識がある．\nこの分野では画像の回転やスケーリングを行うことでデータの拡張を行う．\u003c/p\u003e\n\u003cp\u003eノイズを元データに加えると言う手法もデータ拡張の一つとみなせる．\nニューラルネットワークはノイズに対してあまり頑健でないことが証明されている．\nノイズへの頑健性を高めるためには単純にノイズを加えたデータで学習をする手法がある．\n入力へのノイズの注入は雑音除去自己符号器のような教師なし学習アルゴリズムの一部である．\n2014年にPoole et al.によりノイズの大きさを非常に注意深く調整すれば\nこの手法が極めて有効であることが証明された．\nドロップアウトもノイズの乗算を用いて新しい入力を構成する皇帝とみなせる．\u003c/p\u003e\n\u003cp\u003e機械学習アルゴリズムのベンチマークを比較する際は，\nスコアを算出した際にデータ拡張が行われていたか確かめることが重要である．\nデータ集合の拡張の有無によって差が出ている場合，\n単純にモデルやアルゴリズムの比較ができないためである．\u003c/p\u003e\n\u003ch1 id=\"ノイズに対する頑健性\"\u003eノイズに対する頑健性\u003c/h1\u003e\n\u003cp\u003e一般的に，ノイズの追加は，特にノイズが隠れユニットに加えられた場合に，\n単純にパラメータを縮小するよりずっと強力になりうる．\u003c/p\u003e\n\u003ch3 id=\"出力目標へのノイズの注入\"\u003e出力目標へのノイズの注入\u003c/h3\u003e\n\u003cp\u003eデータ集合の拡張で記述したノイズ手法以外に，出力ラベルに対してノイズを含ませる手法も存在する．\nこのような手法の代表的なものがラベル平滑化と呼ばれる手法である．\u003c/p\u003e\n\u003ch1 id=\"半教師あり学習\"\u003e半教師あり学習\u003c/h1\u003e\n\u003cp\u003e表現を学習する手法であるらしいがあまり理解できなかった．\nChapelle et al, (2006)に詳細が書いてあるらしい．\u003c/p\u003e\n\u003ch1 id=\"マルチタスク学習\"\u003eマルチタスク学習\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003e異なるタスクに関連づけられているデータで観測される変動を説明する因子の中には，\n二つ以上のタスクの間で共有されるものがいくつか存在する\u003c/em\u003e\nと言う事前信念のもとに適用される．\nモデルの一部を異なるタスク間で共有し，重みも共有することで汎化性能を改善する．\nこのようなモデルのパラメータは\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eタスク固有のパラメータ\u003c/li\u003e\n\u003cli\u003e異なるタスク間で共有されるパラメータ\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e汎化と汎化誤差の浄化をこれらのモデルによって改善することが可能である．\u003c/p\u003e\n\u003ch1 id=\"早期終了\"\u003e早期終了\u003c/h1\u003e\n\u003cp\u003eエポック数を回し切った最後のパラメタではなく，\n検証誤差が最少となったエポックでのパラメタを返す手法を早期終了という．\n早期終了はその単純さと有効性から，深層学習において一般的に最も使われている正則化である．\u003c/p\u003e\n\u003cp\u003e最大のメリットとして，ハイパーパラメタの一つであるエポック数を排除できることが挙げられる．\n逆に，デメリットは訓練中に定期的に検証誤差を測定する必要があるということと，\n最良のパラメタのコピーを保持する必要があるという点である．\nしかしパラメタの保持についてはメモリに格納する必要はないため，通常無視することができる．\u003c/p\u003e\n\u003cp\u003e早期終了では検証集合が必要となるため，一部のデータが訓練で使えないこととなる．\nこれらのデータを使いたい場合，早期終了を行い，最適なエポック数やモデル訓練回数を調べた後に，\n同様の回数でもう一度始めから訓練するという手法がある．\nこの場合，パラメタの更新回数を等しくするのかエポック数を等しくするかの2通りの手法がある．\nまた，早期終了で得たパラメタを保持したまま追加分のデータを含めたデータ集合で訓練するという\n手法もある．\nしかし，この手法は一般的に良好な結果を示さない．\u003c/p\u003e\n\u003ch1 id=\"パラメタ拘束とパラメタ共有\"\u003eパラメタ拘束とパラメタ共有\u003c/h1\u003e\n\u003cp\u003e類似したタスクを，類似した入出力をもつ二つのモデルの重み \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eA\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsubsup\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eB\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ew_i^{(A)}, w_i^{(B)}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.321664em;vertical-align:-0.27686399999999994em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.0448em;\"\u003e\u003cspan style=\"top:-2.4231360000000004em;margin-left:-0.02691em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2198em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\"\u003eA\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.27686399999999994em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mpunct\"\u003e,\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02691em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:1.0448em;\"\u003e\u003cspan style=\"top:-2.4231360000000004em;margin-left:-0.02691em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003ei\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.2198em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mopen mtight\"\u003e(\u003c/span\u003e\u003cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\"\u003eB\u003c/span\u003e\u003cspan class=\"mclose mtight\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.27686399999999994em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\nがあるとき，片方の学習時にもう片方の学習済みパラメタに近づけるように\nノルムペナルティを課すことをパラメタ拘束といい，\n完全に学習済みのパラメタと同じになるように学習することをパラメタ共有という．\u003c/p\u003e\n\u003ch3 id=\"畳み込みニューラルネットワーク\"\u003e畳み込みニューラルネットワーク\u003c/h3\u003e\n\u003cp\u003eパラメタ共有最大の活躍の場は間違いなく畳み込みニューラルネットワークである．\n特に画像認識において1ピクセルのずれても猫は猫である．\nそのため，入力全体に対して同様の重みを持つ隠れユニットを用いて計算することが有効となる．\u003c/p\u003e\n\u003ch1 id=\"スパース表現\"\u003eスパース表現\u003c/h1\u003e\n\u003cp\u003e下記のような正則化を用いることでパラメタのスパース表現が可能になる．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eL\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eL^1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\"\u003eL\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8141079999999999em;\"\u003e\u003cspan style=\"top:-3.063em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e ノルムペナルティ\u003c/li\u003e\n\u003cli\u003eスチューデントのt事前分布から導かれたペナルティ\u003c/li\u003e\n\u003cli\u003eKLダイバージェンスペナルティ\u003c/li\u003e\n\u003cli\u003e直行マッチング追跡\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"バギングやその他のアンサンブル手法\"\u003eバギングやその他のアンサンブル手法\u003c/h1\u003e\n\u003cp\u003eバギング(Bootstrap AGGregatING)はいくつかのモデルを組み合わせることで汎化誤差を\n減少させる手法である．\nモデル平均化と呼ばれる機械学習の一般的な手法や．\nアンサンブル手法などが含まれる？\u003c/p\u003e\n\u003cp\u003e(この辺りの関係はイマイチわかっていないので後で調べるかもしれない)\u003c/p\u003e\n\u003cp\u003eモデル平均化は汎化誤差を削減する手法としては極めて強力で信頼できるものである．\u003c/p\u003e\n\u003ch1 id=\"ドロップアウト\"\u003eドロップアウト\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eドロップアウトは最強である\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eドロップアウトは大規模なニューラルネットワークに対して\nバギングを実用的に行う手法であると考えられる．\n具体的な手法はランダムに隠れユニットからの出力が0になる場所を作成する．\nそして，出力のないユニットが覗かれたランダムなモデルを含んだアンサンブル学習を\n近似的に行っていると見做すことができる．\u003c/p\u003e\n\u003cp\u003eドロップアウトは重み減衰やスパース活動正則化といった標準的で\n計算コストの低い他の正則化手法より効果的であることが示されている．\nただし，訓練事例がニューラルネットワークに用いる標準的な量と比して，\n極端に小さい場合はその限りではないという結果も得られている．\u003c/p\u003e\n\u003cp\u003eまた，ドロップアウトを用いることで各隠れユニットは他の隠れユニットが\nどのようなものであろうとも性能を発揮できるように学習されるという利点もある．\u003c/p\u003e\n\u003ch1 id=\"敵対的学習\"\u003e敵対的学習\u003c/h1\u003e\n\u003cp\u003eニューラルネットワークは高い線型性を持っているため，入力値の小さな変化に対して\n大きく反応してしまうことがある．\n例えばパンダの画像にノイズを混ぜると，\n人間はその画像をただのパンダの画像としか認知できないが，\n元の画像ではパンダと認識できていた深層学習モデルはそれをテナガザルと推論することもある．\nこの事例を逆手に取り，例えばノイズを加えた画像を訓練事例に含めていくことで\n誤り率を減少させることが可能である．\u003c/p\u003e\n\u003ch1 id=\"接距離，接線伝播法，多様体接分類器\"\u003e接距離，接線伝播法，多様体接分類器\u003c/h1\u003e\n\u003cp\u003e次元の呪いを克服するため，機械学習の分野では多様体仮説を元に学習を行うことが多い．\n接距離アルゴリズムと\u0010接線伝播アルゴリズムはその仮説をそのままアルゴリズムに持ち込む．\n特に接線伝播法はデータ集合拡張や二重逆伝播法，敵対的学習などと関係する．\u003c/p\u003e\n\u003ch1 id=\"まとめ\"\u003eまとめ\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eドロップアウトと早期学習を組み合わせておけばなんかうまい感じによしなになりそうな説！\u003c/em\u003e\u003c/p\u003e","ogImage":{"url":"/assets/blog/dynamic-routing/cover.jpg"},"coverImage":"/assets/blog/dynamic-routing/cover.jpg"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2019-06-04-deep_learning_5"},"buildId":"yNrEyh5A0Oyk3eJQw5kY9","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>