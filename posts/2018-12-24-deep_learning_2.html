<!DOCTYPE html><html lang="ja"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#fdf6e3"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#fdf6e3"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta name="description"/><meta property="og:title" content="機械学習の基礎的な知識についてざっくりとしたまとめ"/><meta property="og:type" content="website"/><meta property="og:url" content="{DOMAIN}/posts/2018-12-24-deep_learning_2"/><meta property="og:description"/><meta property="og:image" content="https://og-image.vercel.app/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta property="og:image:alt"/><meta property="og:site_name" content="dondakeshimoの丸太"/><meta property="og:locale" content="ja_JP"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="機械学習の基礎的な知識についてざっくりとしたまとめ"/><meta property="twitter:description"/><meta property="twitter:image" content="https://og-image.vercel.app/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><meta property="twitter:image:alt"/><meta property="twitter:site" content="dondakeshimo"/><meta property="twitter:creator" content="dondakeshimo"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"/><title>機械学習の基礎的な知識についてざっくりとしたまとめ | dondakeshimoの丸太</title><meta property="og:image" content="/assets/blog/dynamic-routing/cover.jpg"/><meta name="next-head-count" content="31"/><link rel="preload" href="/_next/static/css/0f9d9436e0b0760d86ae.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0f9d9436e0b0760d86ae.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a54b4f32bdc1ef890ddd.js"></script><script src="/_next/static/chunks/webpack-61095c13c5984b221292.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-c034215587cd157b2989.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d2622b3552023a29a89a.js" defer=""></script><script src="/_next/static/chunks/cb1608f2-854db26f8877d6c8528f.js" defer=""></script><script src="/_next/static/chunks/a9a7754c-ecd5c258dd80f13aa656.js" defer=""></script><script src="/_next/static/chunks/349-e0275456508bca40c658.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-c414199f8f835e5668ae.js" defer=""></script><script src="/_next/static/yNrEyh5A0Oyk3eJQw5kY9/_buildManifest.js" defer=""></script><script src="/_next/static/yNrEyh5A0Oyk3eJQw5kY9/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-vfull"><main><div class="full-container mx-auto px-5"><a class="text-l font-bold text-tight mb-xl mt-l"><a class="logo" href="/"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="hand-point-up" class="svg-inline--fa fa-hand-point-up fa-w-12 svg-l dondake-icon" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path fill="currentColor" d="M135.652 0c23.625 0 43.826 20.65 43.826 44.8v99.851c17.048-16.34 49.766-18.346 70.944 6.299 22.829-14.288 53.017-2.147 62.315 16.45C361.878 158.426 384 189.346 384 240c0 2.746-.203 13.276-.195 16 .168 61.971-31.065 76.894-38.315 123.731C343.683 391.404 333.599 400 321.786 400H150.261l-.001-.002c-18.366-.011-35.889-10.607-43.845-28.464C93.421 342.648 57.377 276.122 29.092 264 10.897 256.203.008 242.616 0 224c-.014-34.222 35.098-57.752 66.908-44.119 8.359 3.583 16.67 8.312 24.918 14.153V44.8c0-23.45 20.543-44.8 43.826-44.8zM136 416h192c13.255 0 24 10.745 24 24v48c0 13.255-10.745 24-24 24H136c-13.255 0-24-10.745-24-24v-48c0-13.255 10.745-24 24-24zm168 28c-11.046 0-20 8.954-20 20s8.954 20 20 20 20-8.954 20-20-8.954-20-20-20z"></path></svg>dondakeshimoの丸太</a></a><article class="mb-2xl"><h1 class="title text-xl font-bold mb-l text-center">機械学習の基礎的な知識についてざっくりとしたまとめ</h1><div class="container-80 mx-auto"><div class="mb-l text-l"><time dateTime="2018-12-24">December	24, 2018</time></div></div><div class="container-80 mx-auto"><div><h1 id="目次">目次</h1>
<ul>
<li><a href="#%E5%AD%A6%E8%A1%93%E7%9A%84%E3%81%AA%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E5%9F%BA%E7%A4%8E">(学術的な)機械学習の基礎</a></li>
<li><a href="#%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">学習アルゴリズム</a>
<ul>
<li>
<ul>
<li><a href="#%E5%AD%A6%E7%BF%92%E3%81%AE%E5%AE%9A%E7%BE%A9">学習の定義</a></li>
<li><a href="#%E3%82%BF%E3%82%B9%E3%82%AF-t">タスク <em>T</em></a></li>
<li><a href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A8%99-p">性能指標 <em>p</em></a></li>
<li><a href="#%E7%B5%8C%E9%A8%93-e">経験 <em>E</em></a></li>
<li><a href="#%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0">線形回帰</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%AE%B9%E9%87%8F%E3%80%81%E9%81%8E%E5%89%B0%E9%81%A9%E5%90%88%E3%80%81%E6%AD%8C%E5%94%B1%E9%81%A9%E5%90%88">容量、過剰適合、歌唱適合</a>
<ul>
<li>
<ul>
<li><a href="#%E3%83%8E%E3%83%BC%E3%83%95%E3%83%AA%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%81%E5%AE%9A%E7%90%86">ノーフリーランチ定理</a></li>
<li><a href="#%E6%AD%A3%E5%89%87%E5%8C%96">正則化</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%A8%E6%A4%9C%E8%A8%BC%E9%9B%86%E5%90%88">ハイパーパラメータと検証集合</a></li>
<li><a href="#%E6%8E%A8%E5%AE%9A%E9%87%8F%E3%80%81%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E3%80%81%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9">推定量、バイアス、バリアンス</a>
<ul>
<li>
<ul>
<li><a href="#%E7%82%B9%E6%8E%A8%E5%AE%9A%E9%87%8F">点推定量</a></li>
<li><a href="#%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9">バイアス</a></li>
<li><a href="#%E5%88%86%E6%95%A3%E3%81%A8%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE">分散と標準誤差</a></li>
<li><a href="#%E5%B9%B3%E5%9D%87%E4%BA%8C%E4%B9%97%E8%AA%A4%E5%B7%AE%E3%82%92%E6%9C%80%E5%B0%8F%E5%8C%96%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E3%81%A8%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9%E3%81%A8%E3%81%AE%E3%83%88%E3%83%AC%E3%83%BC%E3%83%89%E3%82%AA%E3%83%95">平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A">最尤推定</a></li>
<li><a href="#%E3%83%99%E3%82%A4%E3%82%BA%E7%B5%B1%E8%A8%88">ベイズ統計</a>
<ul>
<li>
<ul>
<li><a href="#%E6%9C%80%E5%A4%A7%E4%BA%8B%E5%BE%8C%E7%A2%BA%E7%8E%87map%E6%8E%A8%E5%AE%9A">最大事後確率(MAP)推定</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">教師あり学習アルゴリズム</a>
<ul>
<li>
<ul>
<li><a href="#%E7%A2%BA%E7%8E%87%E7%9A%84%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92">確率的教師あり学習</a></li>
<li><a href="#%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%83%9E%E3%82%B7%E3%83%B3">サポートベクトルマシン</a></li>
<li><a href="#%E3%81%9D%E3%81%AE%E4%BB%96%E3%81%AE%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">その他の教師あり学習アルゴリズム</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">教師なし学習アルゴリズム</a>
<ul>
<li>
<ul>
<li><a href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca">主成分分析(PCA)</a></li>
<li><a href="#k%E5%B9%B3%E5%9D%87%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%AA%E3%83%B3%E3%82%B0">k平均クラスタリング</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95">確率的勾配降下法</a></li>
<li><a href="#%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89">機械学習アルゴリズムの構築</a></li>
<li><a href="#%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E7%99%BA%E5%B1%95%E3%82%92%E4%BF%83%E3%81%99%E8%AA%B2%E9%A1%8C">深層学習の発展を促す課題</a>
<ul>
<li>
<ul>
<li><a href="#%E6%AC%A1%E5%85%83%E3%81%AE%E5%91%AA%E3%81%84">次元の呪い</a></li>
<li><a href="#%E5%B1%80%E6%89%80%E4%B8%80%E6%A7%98%E3%81%A8%E5%B9%B3%E6%BB%91%E5%8C%96">局所一様と平滑化</a></li>
<li><a href="#%E5%A4%9A%E6%A7%98%E4%BD%93%E5%AD%A6%E7%BF%92">多様体学習</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="学術的な機械学習の基礎">(学術的な)機械学習の基礎</h1>
<p>現在、深層学習について勉強中です。
読んでいる書籍は以下になります。</p>
<p><a href="https://www.amazon.co.jp/dp/4048930621/">深層学習</a></p>
<p>今回、第一部がようやく終わったということで、少し振り返ります。
第一部では下記エントリーの数学の基礎部分と数値計算の基礎、
最後に機械学習の基礎について学びました。</p>
<p><a href="/blog/deep-learning-1">機械学習に必要な数学についてざっくりとしたまとめ</a></p>
<p>機械学習の基礎についてはkaggleに代表されるような技術的なものでも、
企業で使われているような実際的なものでもなく、
あくまで理論的なものになります。
そのため確率やら統計やらの話ばかりで多少イメージしづらかったですが、
論文等を読む際のしっかりとした基本ができたかと思います。
自分のために非常にざっくりと忘備録をつけていきます。</p>
<h1 id="学習アルゴリズム">学習アルゴリズム</h1>
<h3 id="学習の定義">学習の定義</h3>
<blockquote>
<p>「コンピュータプログラムは、性能指標Pで測定されるタスクTにおける性能が
経験Eにより改善される場合、そのタスクTのクラスおよび性能指標Pに関して
経験Eから学習すると言われている」</p>
<p>Mitchell(1997)</p>
</blockquote>
<h3 id="タスク-t">タスク <em>T</em></h3>
<p>一般的なタスクを列挙する</p>
<ul>
<li>分類</li>
<li>欠損値のある入力の分類</li>
<li>回帰</li>
<li>転写</li>
<li>機械翻訳</li>
<li>構造出力</li>
<li>異常検知</li>
<li>合成とサンプリング</li>
<li>欠損値補完</li>
<li>ノイズ除去</li>
<li>密度推定</li>
</ul>
<h3 id="性能指標-p">性能指標 <em>p</em></h3>
<p>機械学習アルゴリズムの能力を評価するためのものが必要である。
例えば、モデルの精度や誤差率などがこれに該当する。
学習に用いた訓練集合とは別のテスト集合を用いて評価する。</p>
<h3 id="経験-e">経験 <em>E</em></h3>
<p>機械学習アルゴリズムは大きく <em>教師あり学習</em> と <em>教師なし学習</em> に
分類されるが、それらの厳格な境界や定義はない。
学習アルゴリズムはデータ集合を経験することで学習する。</p>
<h3 id="線形回帰">線形回帰</h3>
<p>学習アルゴリズムの中でシンプルで有名な例として線形回帰を取り上げる。</p>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi mathvariant="bold">w</mi><mi mathvariant="normal">T</mi></msup><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\hat{y} = \mathbf{w}^{\mathrm{T}} \mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8913309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span><span class="mord mathbf">x</span></span></span></span></span></div>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">w</mi></mrow><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span></span></span></span> は重みと呼ばれるパラメータであり、
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord mathbf">x</span></span></span></span></span> は特徴量である。
訓練集合において、予測と正解の <em>平均二乗誤差</em> を最小化するような重みを求めることで、
データ集合からモデルが経験するということになる。
線形の簡単なモデルを扱っているため、微分方程式が正規方程式として知られる形になり、
一意に解が求まることがわかっている。</p>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msup><mi mathvariant="bold">w</mi><mi mathvariant="normal">T</mi></msup><mi mathvariant="bold">x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">\hat{y} = \mathbf{w}^{\mathrm{T}} \mathbf{x} + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9746609999999999em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">T</span></span></span></span></span></span></span></span></span><span class="mord mathbf">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></span></div>
<p>上記のようにインターセプト項 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></span> を加えたものを線形回帰として扱うこともある。
この場合、 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span></span> はバイアスと呼ばれる。統計のバイアスとは異なる。</p>
<h1 id="容量、過剰適合、歌唱適合">容量、過剰適合、歌唱適合</h1>
<p>この章は汎化に対する話となる。
訓練集合から訓練誤差が最小になるように学習を行うのに対し、
汎化性能を測定するためにはテスト集合から汎化誤差(テスト誤差)を計算する。
訓練集合とテスト集合についてはi.i.d.仮定をおいた上で、学習アルゴリズムが目指すことは</p>
<ol>
<li>訓練誤差を小さくする</li>
<li>訓練誤差とテスト誤差の差を小さくする</li>
</ol>
<p>この二つの要素は機械学習における二つの中心的な課題に相当し、
それぞれ <em>過少適合</em> 、 <em>過剰適合</em> と呼ばれる。
これらの度合いは <em>モデルの容量</em> を変化させることで制御する。
任意の高い容量という最も極端な状態に到達するには、 <em>ノンパラメトリックモデル</em>
の概念を導入する。パラメータが存在しないため、モデルの容量を縛る要素が存在しないモデルである。</p>
<h3 id="ノーフリーランチ定理">ノーフリーランチ定理</h3>
<p>データを生成する分布全てを平均すると、
どのアルゴリズムも過去に観測されていない点を分類する際の誤差率は同じになる
という定理。
全ての分布を平均した場合、他の機械学習アルゴリズムよりも普遍的に良いと言える
機械学習アルゴリズムは存在しないと主張している。</p>
<h3 id="正則化">正則化</h3>
<p>モデルの容量を変更する以外で過少適合、過剰適合を制御する方法として <em>正則化</em>
があげられる。
正則化とは他の解に対して優先度を表現する方法の総称である。
一般的には正則化項と呼ばれるペナルティをコスト関数に追加することでモデルを正則化する。</p>
<h1 id="ハイパーパラメータと検証集合">ハイパーパラメータと検証集合</h1>
<p>ほとんどの機械学習アルゴリズムにおいて、挙動を制御するための設定値が存在し、
それらは <em>ハイパーパラメータ</em> と呼ばれる。
最適なハイパーパラメータを選ぶために <em>検証集合</em> が必要になる。
検証集合は訓練データから構成される。</p>
<p>ほとんどのデータ集合は訓練集合とテスト集合に分けられているが、
その中の訓練集合を訓練集合と検証集合に分割する。
訓練集合でモデルを学習、検証集合でハイパーパラメータの最適化を行い、
テスト集合でモデルの汎化性能を測定する。</p>
<h1 id="推定量、バイアス、バリアンス">推定量、バイアス、バリアンス</h1>
<h3 id="点推定量">点推定量</h3>
<p>関心のある量について「最良の」予測を一つ提示する試みである。</p>
<h3 id="バイアス">バイアス</h3>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>m</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="bold">E</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>m</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">\mathrm{bias}(\hat{\theta}_m) = \mathbf{E}(\hat{\theta}_m) - \theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2078799999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">bias</span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2078799999999998em;vertical-align:-0.25em;"></span><span class="mord mathbf">E</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span></div>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><msub><mi>θ</mi><mi>m</mi></msub><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathrm{bias}(\hat{\theta_m}) = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2078799999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">bias</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span></span> になるものは不偏と呼ばれ、好まれる推定量である。</p>
<h3 id="分散と標準誤差">分散と標準誤差</h3>
<p>推定量のバリアンスとは単に推定量の分散である。
推定量の分散や標準誤差は、潜在的なデータ生成過程からデータ集合を別々に再サンプリングする際に、
データから計算した推定量がどのように変化するかを示す尺度を提供する。</p>
<h3 id="平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ">平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ</h3>
<p>バイアスは関数やパラメータの真の値からの期待偏差を測定する。
バリアンスはデータのサンプル化の方法に起因すると考えられる期待推定力の偏差を測定する。
平均二乗誤差はこれら二つの誤差を組み込んだものである。</p>
<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">M</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">E</mi></mrow><mo>=</mo><mi mathvariant="bold">E</mi><mo stretchy="false">[</mo><mo stretchy="false">(</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>m</mi></msub><mo>−</mo><mi>θ</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">]</mo><mo>=</mo><mrow><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>m</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{MSE} = \mathbf{E}[(\hat{\theta}_m - \theta)^2] = \mathrm{bias}(\hat{\theta}_m)^2 + \mathrm{Var}(\hat{\theta}_m)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord"><span class="mord mathrm">MSE</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2078799999999998em;vertical-align:-0.25em;"></span><span class="mord mathbf">E</span><span class="mopen">[(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.2078799999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">bias</span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.2078799999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Var</span></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div>
<p>バイアスとバリアンスの関係は、モデルの容量や過少適合、過剰適合という機械学習の概念と
密接に関係している。</p>
<h1 id="最尤推定">最尤推定</h1>
<p>最尤推定を解釈する方法の一つとして、最尤推定は訓練集合で定義される経験分布とモデル分布の差を
最小化するとみなす方法がある。
最尤法は負の対数尤度(NLL)の最小化、交差エントロピーの最小化、KLダイバージェンスの最大化
として捉えることができる。</p>
<h1 id="ベイズ統計">ベイズ統計</h1>
<p>点推定に対し、予測を行う際に <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span> の取りうる値全てを考慮するものが考えられる。
この手法はベイズ統計の分野となる。
事前確率分布を先験的に仮定し、事例の経験によって、その分布を書き換えていくことで
値の分布を推定する。</p>
<h3 id="最大事後確率map推定">最大事後確率(MAP)推定</h3>
<p>パラメータの完全なベイズ事後分布を用いた推定を行うことが最も理にかなった手法ではあるが、
点推定を行うことが望ましい場合も多い。これを行う方法の一つとしてMAP推定が挙げられる。</p>
<h1 id="教師あり学習アルゴリズム">教師あり学習アルゴリズム</h1>
<h3 id="確率的教師あり学習">確率的教師あり学習</h3>
<p>最終的な出力を0~1の値に押し込むことで確率を予測することができる。</p>
<h3 id="サポートベクトルマシン">サポートベクトルマシン</h3>
<p>線形回帰をベースとして、カーネルトリックと呼ばれる技法を用いることで、
非常に高い性能を発揮した手法。
データ集合が大きい場合、訓練の計算コストが高くなってしまうというデメリットがある。</p>
<h3 id="その他の教師あり学習アルゴリズム">その他の教師あり学習アルゴリズム</h3>
<ul>
<li>k近傍法</li>
<li>決定木</li>
</ul>
<h1 id="教師なし学習アルゴリズム">教師なし学習アルゴリズム</h1>
<p>古典的な教師なし学習はデータの、より単純な表現を探す手法ということができる。
より単純なに関する定義として</p>
<ul>
<li>より低次元な表現</li>
<li>疎な表現</li>
<li>独立した表現</li>
</ul>
<p>の三つが挙げられる。</p>
<h3 id="主成分分析pca">主成分分析(PCA)</h3>
<p>元の入力よりも次元が低い表現を学習し、
また成分が互いに線形な相関を持たない表現 (独立ではない)を学習する。
詳細は割愛。</p>
<h3 id="k平均クラスタリング">k平均クラスタリング</h3>
<p>k個のクラスタに分割し、疎な表現を実現する。
詳細は割愛。</p>
<h1 id="確率的勾配降下法">確率的勾配降下法</h1>
<p>1回のステップに対し、訓練集合の全てを用いては計算コストが大きくなってしまう。
したがって、アルゴリズムの各ステップにおいて、
訓練集合から一様に抽出されるサンプルのミニバッチをサンプリングし、
それら数百のデータを用いて1回の学習ステップを行うことで計算時間を軽減する。
この手法を確率的勾配降下法と呼び、
更新毎の計算量が訓練集合の大きさに依存しないというメリットを得る。</p>
<h1 id="機械学習アルゴリズムの構築">機械学習アルゴリズムの構築</h1>
<ul>
<li>データ集合の仕様</li>
<li>コスト関数</li>
<li>最適化手順</li>
<li>モデル</li>
</ul>
<p>上記4つを組み合わせることで機械学習アルゴリズムは構築される。</p>
<h1 id="深層学習の発展を促す課題">深層学習の発展を促す課題</h1>
<h3 id="次元の呪い">次元の呪い</h3>
<p>次元が大きくなると構成される空間が大きくなりすぎる。</p>
<h3 id="局所一様と平滑化">局所一様と平滑化</h3>
<p>機械学習アルゴリズムにおいて、ある正解データの近傍にあるものは
その正解データと大きく変わらない値に学習されるように、
事前分布が仕込まれていることがほとんである。
その場合、正解データが多く集まっている場所に関しては極めてよく学習できても、
それ以外のデータが集まっていないだけで、同様の分布を持っている場所については
正しく予測できる保証がない。</p>
<h3 id="多様体学習">多様体学習</h3>
<p>多様体は連結した領域であり、各店の周りの近傍に関連づけられた点の集合である。
ある表現の整理の仕方によって次元が変わるということだと思われる。</p></div></div></article></div></main></div><footer><div class="full-container mx-auto px-5"><div class="full-container mx-auto px-5"><div class="py-m sns-container my-l"><div class="mx-s github-icon"><a href="https://github.com/dondakeshimo"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div><div class="mx-s twitter-icon"><a href="https://twitter.com/dondakeshimo"><svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="twitter" class="svg-inline--fa fa-twitter fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div></div></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"機械学習の基礎的な知識についてざっくりとしたまとめ","date":"2018-12-24","slug":"2018-12-24-deep_learning_2","author":{"name":"JJ Kasper","picture":"/assets/blog/authors/jj.jpeg"},"content":"\u003ch1 id=\"目次\"\u003e目次\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E5%AD%A6%E8%A1%93%E7%9A%84%E3%81%AA%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E5%9F%BA%E7%A4%8E\"\u003e(学術的な)機械学習の基礎\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0\"\u003e学習アルゴリズム\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E5%AD%A6%E7%BF%92%E3%81%AE%E5%AE%9A%E7%BE%A9\"\u003e学習の定義\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%82%BF%E3%82%B9%E3%82%AF-t\"\u003eタスク \u003cem\u003eT\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A8%99-p\"\u003e性能指標 \u003cem\u003ep\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E7%B5%8C%E9%A8%93-e\"\u003e経験 \u003cem\u003eE\u003c/em\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0\"\u003e線形回帰\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%AE%B9%E9%87%8F%E3%80%81%E9%81%8E%E5%89%B0%E9%81%A9%E5%90%88%E3%80%81%E6%AD%8C%E5%94%B1%E9%81%A9%E5%90%88\"\u003e容量、過剰適合、歌唱適合\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%8E%E3%83%BC%E3%83%95%E3%83%AA%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%81%E5%AE%9A%E7%90%86\"\u003eノーフリーランチ定理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%AD%A3%E5%89%87%E5%8C%96\"\u003e正則化\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%A8%E6%A4%9C%E8%A8%BC%E9%9B%86%E5%90%88\"\u003eハイパーパラメータと検証集合\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%8E%A8%E5%AE%9A%E9%87%8F%E3%80%81%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E3%80%81%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9\"\u003e推定量、バイアス、バリアンス\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E7%82%B9%E6%8E%A8%E5%AE%9A%E9%87%8F\"\u003e点推定量\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9\"\u003eバイアス\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%88%86%E6%95%A3%E3%81%A8%E6%A8%99%E6%BA%96%E8%AA%A4%E5%B7%AE\"\u003e分散と標準誤差\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%B9%B3%E5%9D%87%E4%BA%8C%E4%B9%97%E8%AA%A4%E5%B7%AE%E3%82%92%E6%9C%80%E5%B0%8F%E5%8C%96%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E3%81%A8%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9%E3%81%A8%E3%81%AE%E3%83%88%E3%83%AC%E3%83%BC%E3%83%89%E3%82%AA%E3%83%95\"\u003e平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%9C%80%E5%B0%A4%E6%8E%A8%E5%AE%9A\"\u003e最尤推定\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%83%99%E3%82%A4%E3%82%BA%E7%B5%B1%E8%A8%88\"\u003eベイズ統計\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E6%9C%80%E5%A4%A7%E4%BA%8B%E5%BE%8C%E7%A2%BA%E7%8E%87map%E6%8E%A8%E5%AE%9A\"\u003e最大事後確率(MAP)推定\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0\"\u003e教師あり学習アルゴリズム\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E7%A2%BA%E7%8E%87%E7%9A%84%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92\"\u003e確率的教師あり学習\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%83%9E%E3%82%B7%E3%83%B3\"\u003eサポートベクトルマシン\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E3%81%9D%E3%81%AE%E4%BB%96%E3%81%AE%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0\"\u003eその他の教師あり学習アルゴリズム\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0\"\u003e教師なし学習アルゴリズム\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca\"\u003e主成分分析(PCA)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#k%E5%B9%B3%E5%9D%87%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%AA%E3%83%B3%E3%82%B0\"\u003ek平均クラスタリング\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95\"\u003e確率的勾配降下法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E6%A7%8B%E7%AF%89\"\u003e機械学習アルゴリズムの構築\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E7%99%BA%E5%B1%95%E3%82%92%E4%BF%83%E3%81%99%E8%AA%B2%E9%A1%8C\"\u003e深層学習の発展を促す課題\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#%E6%AC%A1%E5%85%83%E3%81%AE%E5%91%AA%E3%81%84\"\u003e次元の呪い\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%B1%80%E6%89%80%E4%B8%80%E6%A7%98%E3%81%A8%E5%B9%B3%E6%BB%91%E5%8C%96\"\u003e局所一様と平滑化\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#%E5%A4%9A%E6%A7%98%E4%BD%93%E5%AD%A6%E7%BF%92\"\u003e多様体学習\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"学術的な機械学習の基礎\"\u003e(学術的な)機械学習の基礎\u003c/h1\u003e\n\u003cp\u003e現在、深層学習について勉強中です。\n読んでいる書籍は以下になります。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.amazon.co.jp/dp/4048930621/\"\u003e深層学習\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e今回、第一部がようやく終わったということで、少し振り返ります。\n第一部では下記エントリーの数学の基礎部分と数値計算の基礎、\n最後に機械学習の基礎について学びました。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/deep-learning-1\"\u003e機械学習に必要な数学についてざっくりとしたまとめ\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e機械学習の基礎についてはkaggleに代表されるような技術的なものでも、\n企業で使われているような実際的なものでもなく、\nあくまで理論的なものになります。\nそのため確率やら統計やらの話ばかりで多少イメージしづらかったですが、\n論文等を読む際のしっかりとした基本ができたかと思います。\n自分のために非常にざっくりと忘備録をつけていきます。\u003c/p\u003e\n\u003ch1 id=\"学習アルゴリズム\"\u003e学習アルゴリズム\u003c/h1\u003e\n\u003ch3 id=\"学習の定義\"\u003e学習の定義\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e「コンピュータプログラムは、性能指標Pで測定されるタスクTにおける性能が\n経験Eにより改善される場合、そのタスクTのクラスおよび性能指標Pに関して\n経験Eから学習すると言われている」\u003c/p\u003e\n\u003cp\u003eMitchell(1997)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"タスク-t\"\u003eタスク \u003cem\u003eT\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003e一般的なタスクを列挙する\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e分類\u003c/li\u003e\n\u003cli\u003e欠損値のある入力の分類\u003c/li\u003e\n\u003cli\u003e回帰\u003c/li\u003e\n\u003cli\u003e転写\u003c/li\u003e\n\u003cli\u003e機械翻訳\u003c/li\u003e\n\u003cli\u003e構造出力\u003c/li\u003e\n\u003cli\u003e異常検知\u003c/li\u003e\n\u003cli\u003e合成とサンプリング\u003c/li\u003e\n\u003cli\u003e欠損値補完\u003c/li\u003e\n\u003cli\u003eノイズ除去\u003c/li\u003e\n\u003cli\u003e密度推定\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"性能指標-p\"\u003e性能指標 \u003cem\u003ep\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003e機械学習アルゴリズムの能力を評価するためのものが必要である。\n例えば、モデルの精度や誤差率などがこれに該当する。\n学習に用いた訓練集合とは別のテスト集合を用いて評価する。\u003c/p\u003e\n\u003ch3 id=\"経験-e\"\u003e経験 \u003cem\u003eE\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003e機械学習アルゴリズムは大きく \u003cem\u003e教師あり学習\u003c/em\u003e と \u003cem\u003e教師なし学習\u003c/em\u003e に\n分類されるが、それらの厳格な境界や定義はない。\n学習アルゴリズムはデータ集合を経験することで学習する。\u003c/p\u003e\n\u003ch3 id=\"線形回帰\"\u003e線形回帰\u003c/h3\u003e\n\u003cp\u003e学習アルゴリズムの中でシンプルで有名な例として線形回帰を取り上げる。\u003c/p\u003e\n\u003cdiv class=\"math math-display\"\u003e\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsup\u003e\u003cmi mathvariant=\"bold\"\u003ew\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi mathvariant=\"bold\"\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{y} = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.69444em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.19444em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.19444em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8913309999999999em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathbf\" style=\"margin-right:0.01597em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8913309999999999em;\"\u003e\u003cspan style=\"top:-3.113em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathrm mtight\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/div\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"bold\"\u003ew\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathbf{w}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\" style=\"margin-right:0.01597em;\"\u003ew\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e は重みと呼ばれるパラメータであり、\n\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi mathvariant=\"bold\"\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathbf{x}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003ex\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e は特徴量である。\n訓練集合において、予測と正解の \u003cem\u003e平均二乗誤差\u003c/em\u003e を最小化するような重みを求めることで、\nデータ集合からモデルが経験するということになる。\n線形の簡単なモデルを扱っているため、微分方程式が正規方程式として知られる形になり、\n一意に解が求まることがわかっている。\u003c/p\u003e\n\u003cdiv class=\"math math-display\"\u003e\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003ey\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsup\u003e\u003cmi mathvariant=\"bold\"\u003ew\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi mathvariant=\"bold\"\u003ex\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\hat{y} = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x} + b\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.69444em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\"\u003ey\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.19444em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.19444em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.9746609999999999em;vertical-align:-0.08333em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathbf\" style=\"margin-right:0.01597em;\"\u003ew\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8913309999999999em;\"\u003e\u003cspan style=\"top:-3.113em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e\u003cspan class=\"mord mathrm mtight\"\u003eT\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003ex\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/div\u003e\n\u003cp\u003e上記のようにインターセプト項 \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eb\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e を加えたものを線形回帰として扱うこともある。\nこの場合、 \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eb\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eb\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003eb\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e はバイアスと呼ばれる。統計のバイアスとは異なる。\u003c/p\u003e\n\u003ch1 id=\"容量、過剰適合、歌唱適合\"\u003e容量、過剰適合、歌唱適合\u003c/h1\u003e\n\u003cp\u003eこの章は汎化に対する話となる。\n訓練集合から訓練誤差が最小になるように学習を行うのに対し、\n汎化性能を測定するためにはテスト集合から汎化誤差(テスト誤差)を計算する。\n訓練集合とテスト集合についてはi.i.d.仮定をおいた上で、学習アルゴリズムが目指すことは\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e訓練誤差を小さくする\u003c/li\u003e\n\u003cli\u003e訓練誤差とテスト誤差の差を小さくする\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこの二つの要素は機械学習における二つの中心的な課題に相当し、\nそれぞれ \u003cem\u003e過少適合\u003c/em\u003e 、 \u003cem\u003e過剰適合\u003c/em\u003e と呼ばれる。\nこれらの度合いは \u003cem\u003eモデルの容量\u003c/em\u003e を変化させることで制御する。\n任意の高い容量という最も極端な状態に到達するには、 \u003cem\u003eノンパラメトリックモデル\u003c/em\u003e\nの概念を導入する。パラメータが存在しないため、モデルの容量を縛る要素が存在しないモデルである。\u003c/p\u003e\n\u003ch3 id=\"ノーフリーランチ定理\"\u003eノーフリーランチ定理\u003c/h3\u003e\n\u003cp\u003eデータを生成する分布全てを平均すると、\nどのアルゴリズムも過去に観測されていない点を分類する際の誤差率は同じになる\nという定理。\n全ての分布を平均した場合、他の機械学習アルゴリズムよりも普遍的に良いと言える\n機械学習アルゴリズムは存在しないと主張している。\u003c/p\u003e\n\u003ch3 id=\"正則化\"\u003e正則化\u003c/h3\u003e\n\u003cp\u003eモデルの容量を変更する以外で過少適合、過剰適合を制御する方法として \u003cem\u003e正則化\u003c/em\u003e\nがあげられる。\n正則化とは他の解に対して優先度を表現する方法の総称である。\n一般的には正則化項と呼ばれるペナルティをコスト関数に追加することでモデルを正則化する。\u003c/p\u003e\n\u003ch1 id=\"ハイパーパラメータと検証集合\"\u003eハイパーパラメータと検証集合\u003c/h1\u003e\n\u003cp\u003eほとんどの機械学習アルゴリズムにおいて、挙動を制御するための設定値が存在し、\nそれらは \u003cem\u003eハイパーパラメータ\u003c/em\u003e と呼ばれる。\n最適なハイパーパラメータを選ぶために \u003cem\u003e検証集合\u003c/em\u003e が必要になる。\n検証集合は訓練データから構成される。\u003c/p\u003e\n\u003cp\u003eほとんどのデータ集合は訓練集合とテスト集合に分けられているが、\nその中の訓練集合を訓練集合と検証集合に分割する。\n訓練集合でモデルを学習、検証集合でハイパーパラメータの最適化を行い、\nテスト集合でモデルの汎化性能を測定する。\u003c/p\u003e\n\u003ch1 id=\"推定量、バイアス、バリアンス\"\u003e推定量、バイアス、バリアンス\u003c/h1\u003e\n\u003ch3 id=\"点推定量\"\u003e点推定量\u003c/h3\u003e\n\u003cp\u003e関心のある量について「最良の」予測を一つ提示する試みである。\u003c/p\u003e\n\u003ch3 id=\"バイアス\"\u003eバイアス\u003c/h3\u003e\n\u003cdiv class=\"math math-display\"\u003e\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eb\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ei\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ea\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003es\u003c/mi\u003e\u003c/mrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eE\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathrm{bias}(\\hat{\\theta}_m) = \\mathbf{E}(\\hat{\\theta}_m) - \\theta\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\"\u003ebias\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9578799999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.26344em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.16666em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.151392em;\"\u003e\u003cspan style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003eE\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9578799999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.26344em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.16666em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.151392em;\"\u003e\u003cspan style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/div\u003e\n\u003cp\u003e\u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eb\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ei\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ea\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003es\u003c/mi\u003e\u003c/mrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmover accent=\"true\"\u003e\u003cmsub\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathrm{bias}(\\hat{\\theta_m}) = 0\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\"\u003ebias\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9578799999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.151392em;\"\u003e\u003cspan style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.26344em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.25em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e0\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e になるものは不偏と呼ばれ、好まれる推定量である。\u003c/p\u003e\n\u003ch3 id=\"分散と標準誤差\"\u003e分散と標準誤差\u003c/h3\u003e\n\u003cp\u003e推定量のバリアンスとは単に推定量の分散である。\n推定量の分散や標準誤差は、潜在的なデータ生成過程からデータ集合を別々に再サンプリングする際に、\nデータから計算した推定量がどのように変化するかを示す尺度を提供する。\u003c/p\u003e\n\u003ch3 id=\"平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ\"\u003e平均二乗誤差を最小化するためのバイアスとバリアンスとのトレードオフ\u003c/h3\u003e\n\u003cp\u003eバイアスは関数やパラメータの真の値からの期待偏差を測定する。\nバリアンスはデータのサンプル化の方法に起因すると考えられる期待推定力の偏差を測定する。\n平均二乗誤差はこれら二つの誤差を組み込んだものである。\u003c/p\u003e\n\u003cdiv class=\"math math-display\"\u003e\u003cspan class=\"katex-display\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eM\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003eS\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003eE\u003c/mi\u003e\u003c/mrow\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi mathvariant=\"bold\"\u003eE\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e[\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e]\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eb\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ei\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ea\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003es\u003c/mi\u003e\u003c/mrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/msub\u003e\u003cmsup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmrow\u003e\u003cmi mathvariant=\"normal\"\u003eV\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003ea\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003er\u003c/mi\u003e\u003c/mrow\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmover accent=\"true\"\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003cmo\u003e^\u003c/mo\u003e\u003c/mover\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\mathrm{MSE} = \\mathbf{E}[(\\hat{\\theta}_m - \\theta)^2] = \\mathrm{bias}(\\hat{\\theta}_m)^2 + \\mathrm{Var}(\\hat{\\theta}_m)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\"\u003eMSE\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathbf\"\u003eE\u003c/span\u003e\u003cspan class=\"mopen\"\u003e[(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9578799999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.26344em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.16666em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.151392em;\"\u003e\u003cspan style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e−\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003cspan class=\"mclose\"\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8641079999999999em;\"\u003e\u003cspan style=\"top:-3.113em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e]\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\"\u003ebias\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9578799999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.26344em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.16666em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.151392em;\"\u003e\u003cspan style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.8641079999999999em;\"\u003e\u003cspan style=\"top:-3.113em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mtight\"\u003e2\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003cspan class=\"mbin\"\u003e+\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord mathrm\"\u003eVar\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mopen\"\u003e(\u003c/span\u003e\u003cspan class=\"mord\"\u003e\u003cspan class=\"mord accent\"\u003e\u003cspan class=\"vlist-t\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.9578799999999998em;\"\u003e\u003cspan style=\"top:-3em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"top:-3.26344em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:3em;\"\u003e\u003c/span\u003e\u003cspan class=\"accent-body\" style=\"left:-0.16666em;\"\u003e\u003cspan class=\"mord\"\u003e^\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"msupsub\"\u003e\u003cspan class=\"vlist-t vlist-t2\"\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.151392em;\"\u003e\u003cspan style=\"top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;\"\u003e\u003cspan class=\"pstrut\" style=\"height:2.7em;\"\u003e\u003c/span\u003e\u003cspan class=\"sizing reset-size6 size3 mtight\"\u003e\u003cspan class=\"mord mathnormal mtight\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-s\"\u003e​\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"vlist-r\"\u003e\u003cspan class=\"vlist\" style=\"height:0.15em;\"\u003e\u003cspan\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"mclose\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/div\u003e\n\u003cp\u003eバイアスとバリアンスの関係は、モデルの容量や過少適合、過剰適合という機械学習の概念と\n密接に関係している。\u003c/p\u003e\n\u003ch1 id=\"最尤推定\"\u003e最尤推定\u003c/h1\u003e\n\u003cp\u003e最尤推定を解釈する方法の一つとして、最尤推定は訓練集合で定義される経験分布とモデル分布の差を\n最小化するとみなす方法がある。\n最尤法は負の対数尤度(NLL)の最小化、交差エントロピーの最小化、KLダイバージェンスの最大化\nとして捉えることができる。\u003c/p\u003e\n\u003ch1 id=\"ベイズ統計\"\u003eベイズ統計\u003c/h1\u003e\n\u003cp\u003e点推定に対し、予測を行う際に \u003cspan class=\"math math-inline\"\u003e\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eθ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\theta\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\"\u003eθ\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e の取りうる値全てを考慮するものが考えられる。\nこの手法はベイズ統計の分野となる。\n事前確率分布を先験的に仮定し、事例の経験によって、その分布を書き換えていくことで\n値の分布を推定する。\u003c/p\u003e\n\u003ch3 id=\"最大事後確率map推定\"\u003e最大事後確率(MAP)推定\u003c/h3\u003e\n\u003cp\u003eパラメータの完全なベイズ事後分布を用いた推定を行うことが最も理にかなった手法ではあるが、\n点推定を行うことが望ましい場合も多い。これを行う方法の一つとしてMAP推定が挙げられる。\u003c/p\u003e\n\u003ch1 id=\"教師あり学習アルゴリズム\"\u003e教師あり学習アルゴリズム\u003c/h1\u003e\n\u003ch3 id=\"確率的教師あり学習\"\u003e確率的教師あり学習\u003c/h3\u003e\n\u003cp\u003e最終的な出力を0~1の値に押し込むことで確率を予測することができる。\u003c/p\u003e\n\u003ch3 id=\"サポートベクトルマシン\"\u003eサポートベクトルマシン\u003c/h3\u003e\n\u003cp\u003e線形回帰をベースとして、カーネルトリックと呼ばれる技法を用いることで、\n非常に高い性能を発揮した手法。\nデータ集合が大きい場合、訓練の計算コストが高くなってしまうというデメリットがある。\u003c/p\u003e\n\u003ch3 id=\"その他の教師あり学習アルゴリズム\"\u003eその他の教師あり学習アルゴリズム\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ek近傍法\u003c/li\u003e\n\u003cli\u003e決定木\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"教師なし学習アルゴリズム\"\u003e教師なし学習アルゴリズム\u003c/h1\u003e\n\u003cp\u003e古典的な教師なし学習はデータの、より単純な表現を探す手法ということができる。\nより単純なに関する定義として\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eより低次元な表現\u003c/li\u003e\n\u003cli\u003e疎な表現\u003c/li\u003e\n\u003cli\u003e独立した表現\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eの三つが挙げられる。\u003c/p\u003e\n\u003ch3 id=\"主成分分析pca\"\u003e主成分分析(PCA)\u003c/h3\u003e\n\u003cp\u003e元の入力よりも次元が低い表現を学習し、\nまた成分が互いに線形な相関を持たない表現 (独立ではない)を学習する。\n詳細は割愛。\u003c/p\u003e\n\u003ch3 id=\"k平均クラスタリング\"\u003ek平均クラスタリング\u003c/h3\u003e\n\u003cp\u003ek個のクラスタに分割し、疎な表現を実現する。\n詳細は割愛。\u003c/p\u003e\n\u003ch1 id=\"確率的勾配降下法\"\u003e確率的勾配降下法\u003c/h1\u003e\n\u003cp\u003e1回のステップに対し、訓練集合の全てを用いては計算コストが大きくなってしまう。\nしたがって、アルゴリズムの各ステップにおいて、\n訓練集合から一様に抽出されるサンプルのミニバッチをサンプリングし、\nそれら数百のデータを用いて1回の学習ステップを行うことで計算時間を軽減する。\nこの手法を確率的勾配降下法と呼び、\n更新毎の計算量が訓練集合の大きさに依存しないというメリットを得る。\u003c/p\u003e\n\u003ch1 id=\"機械学習アルゴリズムの構築\"\u003e機械学習アルゴリズムの構築\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eデータ集合の仕様\u003c/li\u003e\n\u003cli\u003eコスト関数\u003c/li\u003e\n\u003cli\u003e最適化手順\u003c/li\u003e\n\u003cli\u003eモデル\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e上記4つを組み合わせることで機械学習アルゴリズムは構築される。\u003c/p\u003e\n\u003ch1 id=\"深層学習の発展を促す課題\"\u003e深層学習の発展を促す課題\u003c/h1\u003e\n\u003ch3 id=\"次元の呪い\"\u003e次元の呪い\u003c/h3\u003e\n\u003cp\u003e次元が大きくなると構成される空間が大きくなりすぎる。\u003c/p\u003e\n\u003ch3 id=\"局所一様と平滑化\"\u003e局所一様と平滑化\u003c/h3\u003e\n\u003cp\u003e機械学習アルゴリズムにおいて、ある正解データの近傍にあるものは\nその正解データと大きく変わらない値に学習されるように、\n事前分布が仕込まれていることがほとんである。\nその場合、正解データが多く集まっている場所に関しては極めてよく学習できても、\nそれ以外のデータが集まっていないだけで、同様の分布を持っている場所については\n正しく予測できる保証がない。\u003c/p\u003e\n\u003ch3 id=\"多様体学習\"\u003e多様体学習\u003c/h3\u003e\n\u003cp\u003e多様体は連結した領域であり、各店の周りの近傍に関連づけられた点の集合である。\nある表現の整理の仕方によって次元が変わるということだと思われる。\u003c/p\u003e","ogImage":{"url":"/assets/blog/dynamic-routing/cover.jpg"},"coverImage":"/assets/blog/dynamic-routing/cover.jpg"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2018-12-24-deep_learning_2"},"buildId":"yNrEyh5A0Oyk3eJQw5kY9","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>